{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from models import *\n",
    "from utils import *\n",
    "\n",
    "import os, sys, time, datetime, random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python==4.2.0.32 in /home/kantorc/.local/lib/python3.5/site-packages (4.2.0.32)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /home/kantorc/.local/lib/python3.5/site-packages (from opencv-python==4.2.0.32) (1.18.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.3.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "4.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install opencv-python==4.2.0.32\n",
    "# before: 4.0.0\n",
    "import cv2\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-contrib-python in /home/kantorc/.local/lib/python3.5/site-packages (4.4.0.42)\n",
      "Requirement already satisfied: numpy>=1.13.1 in /home/kantorc/.local/lib/python3.5/site-packages (from opencv-contrib-python) (1.18.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.3.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install opencv-contrib-python\n",
    "# to undo: pip uninstall opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path='config/yolov3.cfg'\n",
    "weights_path='config/yolov3.weights'\n",
    "class_path='config/coco.names'\n",
    "img_size=416\n",
    "conf_thres=0.8\n",
    "nms_thres=0.4\n",
    "\n",
    "# Load model and weights\n",
    "model = Darknet(config_path, img_size=img_size)\n",
    "model.load_weights(weights_path)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "classes = utils.load_classes(class_path)\n",
    "Tensor = torch.cuda.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_image(img):\n",
    "    # scale and pad image\n",
    "    ratio = min(img_size/img.size[0], img_size/img.size[1])\n",
    "    imw = round(img.size[0] * ratio)\n",
    "    imh = round(img.size[1] * ratio)\n",
    "    img_transforms = transforms.Compose([ transforms.Resize((imh, imw)),\n",
    "         transforms.Pad((max(int((imh-imw)/2),0), max(int((imw-imh)/2),0), max(int((imh-imw)/2),0), max(int((imw-imh)/2),0)),\n",
    "                        (128,128,128)),\n",
    "         transforms.ToTensor(),\n",
    "         ])\n",
    "    # convert image to Tensor\n",
    "    image_tensor = img_transforms(img).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    input_img = Variable(image_tensor.type(Tensor))\n",
    "    # run inference on the model and get detections\n",
    "    with torch.no_grad():\n",
    "        detections = model(input_img)\n",
    "        print\n",
    "        detections = utils.non_max_suppression(detections, 80, conf_thres, nms_thres)\n",
    "    return detections[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kantorc/__SOFFI__/sort'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    }
   ],
   "source": [
    "videopath = '/home/kantorc/__SOFFI__/sort/images/test_video.mp4'\n",
    "\n",
    "%pylab inline \n",
    "import cv2\n",
    "print(cv2.__version__)\n",
    "from IPython.display import clear_output\n",
    "from math import floor\n",
    "cmap = plt.get_cmap('tab20b')\n",
    "colors = [cmap(i)[:3] for i in np.linspace(0, 1, 20)]\n",
    "\n",
    "# initialize Sort object and video capture\n",
    "from sort import *\n",
    "vid = cv2.VideoCapture(videopath)\n",
    "fps = vid.get(cv2.CAP_PROP_FPS)\n",
    "nb_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print (\"Frames per second using video.get(cv2.CAP_PROP_FPS) : {0}\".format(fps))\n",
    "print(f\"Total number of frames{nb_frames}\")\n",
    "\n",
    "# print(vid)\n",
    "mot_tracker = Sort() \n",
    "\n",
    "\n",
    "while vid.isOpened():\n",
    "    frame_width = int(vid.get(3))\n",
    "    frame_height = int(vid.get(4))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter('/home/kantorc/__SOFFI__/sort/results/test.mp4',fourcc, floor(fps), (frame_width,frame_height))\n",
    "    for ii in range(170):\n",
    "        # print(ii)\n",
    "        ret, frame = vid.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pilimg = Image.fromarray(frame)\n",
    "        detections = detect_image(pilimg) \n",
    "        img = np.array(pilimg)\n",
    "        pad_x = max(img.shape[0] - img.shape[1], 0) * (img_size / max(img.shape))\n",
    "        pad_y = max(img.shape[1] - img.shape[0], 0) * (img_size / max(img.shape))\n",
    "        unpad_h = img_size - pad_y\n",
    "        unpad_w = img_size - pad_x\n",
    "        if detections is not None:\n",
    "            tracked_objects = mot_tracker.update(detections.cpu())\n",
    "\n",
    "            unique_labels = detections[:, -1].cpu().unique()\n",
    "            n_cls_preds = len(unique_labels)\n",
    "            for x1, y1, x2, y2, obj_id, cls_pred in tracked_objects:\n",
    "                box_h = int(((y2 - y1) / unpad_h) * img.shape[0])\n",
    "                box_w = int(((x2 - x1) / unpad_w) * img.shape[1])\n",
    "                y1 = int(((y1 - pad_y // 2) / unpad_h) * img.shape[0])\n",
    "                x1 = int(((x1 - pad_x // 2) / unpad_w) * img.shape[1])\n",
    "\n",
    "                color = colors[int(obj_id) % len(colors)]\n",
    "                color = [i * 255 for i in color]\n",
    "                cls = classes[int(cls_pred)]\n",
    "                cv2.rectangle(frame, (x1, y1), (x1+box_w, y1+box_h), color, 4)\n",
    "                cv2.rectangle(frame, (x1, y1-35), (x1+len(cls)*19+60, y1), color, -1)\n",
    "                cv2.putText(frame, cls + \"-\" + str(int(obj_id)), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 3)\n",
    "        out.write(frame)\n",
    "        \n",
    "        fig=figure(figsize=(12, 8))\n",
    "        title(\"Video Stream\")\n",
    "        imshow(frame)\n",
    "        show()\n",
    "        clear_output(wait=True)\n",
    "    out and out.release()\n",
    "    vid.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use object detection algorithm(yolo,ssd,fast rnn) to detect specific category object in your own video,and then you need to save the bounding box and other information given by detector as det.txt's format.\n",
    "det.txt format:\n",
    "\n",
    "frame_id,-1,xmin,ymin,w,h,confidence,-1,-1,-1\n",
    "\n",
    "+ frame_id : number of current frame in frame sequence.\n",
    "+ xmin,ymin,w,h: bounding box of one object\n",
    "+ confidence:score of this detection.\n",
    "+ -1:ignore.You don't need to care this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sort import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 Tensorflow Keras Pytorch (Mohit)",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
